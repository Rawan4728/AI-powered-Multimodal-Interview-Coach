{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   pdfplumber 0.11.6\n",
        "*   docx 1.1.2\n",
        "*   langchain_community 0.3.23\n",
        "\n"
      ],
      "metadata": {
        "id": "pL2jaEm8860B"
      }
    },
    {
      "source": [
        "!pip install pdfplumber"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n96ksLM37-5q",
        "outputId": "7283ab02-7e4c-4ff9-aec7-980a2a201ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m133.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install python-docx # install python-docx using pip"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqhnI8nu9OLH",
        "outputId": "764905b9-0bb8-4c1d-971a-2f297ca9bae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install langchain-community"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKvUpBJZ-8SH",
        "outputId": "34c6c3d4-23c6-4332-cc39-e6ad9bee5a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.38)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community\n",
        "print(langchain_community.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2_0eQ0M9YqO",
        "outputId": "5e41f3d4-e53a-4644-903d-8ec73bbc6c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.23\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install chromadb"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "10kLHr4fDc_V",
        "outputId": "c077ff3f-c877-428b-952a-eda39a68fcd7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.17)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=d8269fbeb89d710f721101a3a1525d5b692640b5bb8be7c4a5c32f9facbb2553\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, uvicorn, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chroma-hnswlib-0.7.6 chromadb-1.0.7 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-32.0.1 mmh3-5.1.0 onnxruntime-1.21.1 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 overrides-7.7.0 posthog-4.0.1 pypika-0.48.9 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "82e0901ff43144f29ef68405d21dd974"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "!pip install tiktoken"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiEq_GEID3Gt",
        "outputId": "f2e8ad2d-edfd-41ce-9477-9b963c57f8d4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Setup"
      ],
      "metadata": {
        "id": "Aj9det5Q_eDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFcQmIeC68-i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import pdfplumber\n",
        "import docx\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up OpenAI"
      ],
      "metadata": {
        "id": "vn9y66ja_rPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "AG6uJ43xANd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "9kRYUrzk_W4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Extract and Clean CV Text"
      ],
      "metadata": {
        "id": "rHgDra64AWti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    text = ''\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + '\\n'\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join(para.text for para in doc.paragraphs)\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "Ge49TUVoAQYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse CV using GPT-4"
      ],
      "metadata": {
        "id": "7ovZLSvlAc5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cv_with_openai(text):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert resume parser.\n",
        "\n",
        "Given the following CV text, extract the following information in JSON format:\n",
        "- Skills (list)\n",
        "- Education (list)\n",
        "- Experience (list: mention years of experience and job titles if possible)\n",
        "\n",
        "CV Text:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "Return the JSON only, no extra text.\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "xdZEuG0aAcoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper to Load and Parse CV"
      ],
      "metadata": {
        "id": "ZDCIrbbIAxyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cv(file_path, file_type='pdf'):\n",
        "    if file_type == 'pdf':\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif file_type == 'docx':\n",
        "        text = extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
        "\n",
        "    text = clean_text(text)\n",
        "    parsed_profile = parse_cv_with_openai(text)\n",
        "    return parsed_profile, text\n"
      ],
      "metadata": {
        "id": "tNHL1yAuAvd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape and Cache Web Best Practices"
      ],
      "metadata": {
        "id": "XNjFAULOA4pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_create_vectorstore(url, persist_directory=\"./chroma_db\"):\n",
        "    index_path = os.path.join(persist_directory, \"index\")\n",
        "    if os.path.exists(index_path):\n",
        "        print(f\"[INFO] Loading vectorstore from {persist_directory}\")\n",
        "        return Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "    print(\"[INFO] Scraping and creating new vectorstore...\")\n",
        "    loader = RecursiveUrlLoader(\n",
        "        url=url,\n",
        "        max_depth=2,\n",
        "        extractor=lambda x: Soup(x, \"html.parser\").text\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=OpenAIEmbeddings(),\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    return vectorstore\n"
      ],
      "metadata": {
        "id": "bJZG-HyCBDJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve Best Practices for a Query"
      ],
      "metadata": {
        "id": "r-ubObMHBLBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_best_practices(query, vectorstore, k=2):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    context = retriever.invoke(query,k=k)\n",
        "    return \" \".join([doc.page_content for doc in context])\n"
      ],
      "metadata": {
        "id": "m7Jd3Ag_BPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Questions using RAG"
      ],
      "metadata": {
        "id": "pgd1h8iPBSSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rag_answer(question, context, parsed_profile=None):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert interview coach.\n",
        "\n",
        "Use the following best practices and candidate profile to answer the question professionally.\n",
        "\n",
        "Context (best practices): {context}\n",
        "\n",
        "Candidate Profile (optional): {parsed_profile}\n",
        "\n",
        "Interview Question: {question}\n",
        "\n",
        "Give a strong example-based answer.\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "w0D_84k-BVnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Interview Questions"
      ],
      "metadata": {
        "id": "U6aUWGv4Ba_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_interview_questions(parsed_profile, num_technical=5, num_behavioral=5):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert HR recruiter.\n",
        "\n",
        "Based on the following candidate profile, generate {num_technical} technical questions and {num_behavioral} behavioral questions.\n",
        "\n",
        "Candidate Profile:\n",
        "{parsed_profile}\n",
        "\n",
        "Rules:\n",
        "- Questions should be specific to the candidate's skills, education, and experience.\n",
        "- Do not make up skills not mentioned.\n",
        "- Return the result as a JSON with two fields: \"technical_questions\" and \"behavioral_questions\".\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "XxAgUCsaBroy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full RAG Pipeline"
      ],
      "metadata": {
        "id": "NOyOeu7YCqgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_workflow(file_path, file_type='pdf', vectorstore=None):\n",
        "    parsed_profile, raw_text = parse_cv(file_path, file_type)\n",
        "\n",
        "    session_data = {\n",
        "        \"filename\": os.path.basename(file_path),\n",
        "        \"uploaded_at\": str(datetime.now()),\n",
        "        \"raw_text\": raw_text,\n",
        "        \"parsed_profile\": parsed_profile\n",
        "    }\n",
        "\n",
        "    questions_json = generate_interview_questions(session_data[\"parsed_profile\"])\n",
        "    question_dict = json.loads(questions_json)\n",
        "\n",
        "    all_answers = {\"technical_answers\": [], \"behavioral_answers\": []}\n",
        "\n",
        "    for q in question_dict[\"behavioral_questions\"]:\n",
        "        context = retrieve_best_practices(q, vectorstore)\n",
        "        answer = generate_rag_answer(q, context, parsed_profile=session_data[\"parsed_profile\"])\n",
        "        all_answers[\"behavioral_answers\"].append({\"question\": q, \"answer\": answer})\n",
        "\n",
        "    for q in question_dict[\"technical_questions\"]:\n",
        "        context = retrieve_best_practices(q, vectorstore)\n",
        "        answer = generate_rag_answer(q, context, parsed_profile=session_data[\"parsed_profile\"])\n",
        "        all_answers[\"technical_answers\"].append({\"question\": q, \"answer\": answer})\n",
        "\n",
        "    return all_answers, session_data\n"
      ],
      "metadata": {
        "id": "CMMBndp2CqNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Everything"
      ],
      "metadata": {
        "id": "gREd5BObCwci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/RawanAlnajim_cv.pdf\"  # Replace with actual path\n",
        "    vectorstore_path = \"./chroma_db\"\n",
        "    advice_url = \"https://www.themuse.com/advice\"\n",
        "\n",
        "    vectorstore = get_or_create_vectorstore(advice_url, persist_directory=vectorstore_path)\n",
        "    answers = run_rag_workflow(file_path, vectorstore=vectorstore)\n",
        "    print(json.dumps(answers, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdOva4HdC0W9",
        "outputId": "27d77168-d6b4-4d5d-9b4c-01d59c099a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Scraping and creating new vectorstore...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"technical_answers\": [\n",
            "      {\n",
            "        \"question\": \"Can you explain a project where you used Python, SQL, CSS, and HTML together?\",\n",
            "        \"answer\": \"Sure, I'd be happy to share an example. During my Data Science and Artificial Intelligence Bootcamp at T5 SDAIA, I was part of a team that worked on a project to develop a web-based application for sentiment analysis on social media posts.\\n\\nSituation: The goal of the project was to create an application that could analyze the sentiment of social media posts in real-time and present the results in an easily understandable format. \\n\\nTask: My task was to build the back-end of the application, which involved data collection, data processing, and implementing the sentiment analysis model. \\n\\nAction: I started by using Python to build a script that could scrape social media data in real-time. I used various Python libraries such as Tweepy for Twitter data and BeautifulSoup for web scraping. Once the data was collected, I used SQL to store and manage this data in a database. This allowed us to keep a record of the data and perform efficient queries when needed. \\n\\nFor the sentiment analysis, I built a machine learning model using Python's Natural Language Processing libraries. Once the model was trained and tested, I integrated it into the data processing pipeline.\\n\\nOn the front-end, my teammates used HTML and CSS to design a user-friendly interface. The front-end would send a request to the back-end, my Python script would then process the request, perform the sentiment analysis, and send the results back to the front-end.\\n\\nResult: The end result was a fully functional web-based application that could analyze the sentiment of social media posts in real-time. Not only did this project allow me to use Python, SQL, CSS, and HTML together, but it also gave me the opportunity to work on a real-world project and apply my data analysis skills. The project was well-received by the bootcamp mentors and my peers, and it was a great learning experience for me.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"What kind of data have you analyzed in the past and what tools did you use for the analysis?\",\n",
            "        \"answer\": \"In my past experiences, I have had the opportunity to analyze a variety of data sets. During my internship at Watad Energy & Communications Co, I worked on a project where I was tasked with analyzing energy consumption data. The goal was to identify patterns and trends that could help the company optimize its energy usage. I used SQL for data extraction and manipulation, and Python, specifically libraries like Pandas and NumPy, for further data analysis.\\n\\nAdditionally, during the Data Science and Artificial Intelligence Bootcamp at T5 SDAIA, I worked on a project that involved analyzing textual data for sentiment analysis. This was part of a larger project aimed at understanding customer feedback for a specific product. For this task, I used Python and its Natural Language Processing (NLP) libraries, such as NLTK and Gensim.\\n\\nIn both cases, I also used Power BI for data visualization to present my findings in a clear and understandable manner. This allowed me to communicate complex data insights to non-technical team members effectively.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"Can you describe a machine learning, deep learning, or natural language processing model you've built?\",\n",
            "        \"answer\": \"During my Data Science and Artificial Intelligence Bootcamp at T5 SDAIA, I had the opportunity to work on a project that involved building a Natural Language Processing (NLP) model. \\n\\nThe model was designed to perform sentiment analysis on social media comments. The goal was to help businesses understand customer sentiment towards their products or services in real-time. This was a challenging project as it involved dealing with unstructured and often ambiguous human language data.\\n\\nI used Python for programming and implemented libraries such as NLTK and Scikit-learn for the NLP tasks. The first step was data collection, where I used a web scraping tool to gather thousands of comments from various social media platforms. After data cleaning and pre-processing, which included tasks like removing stop words and punctuation, tokenization, and lemmatization, I represented the text data numerically using the Bag of Words technique.\\n\\nOnce the data was prepared, I trained a logistic regression model using Scikit-learn. The model was then validated using a separate test dataset. The performance of the model was quite satisfactory, with an accuracy of around 85%.\\n\\nThis project was an excellent learning experience as it allowed me to apply the theoretical knowledge I gained during my bootcamp. It also reinforced my understanding of the end-to-end process of building a machine learning model, from understanding the problem and collecting data to model building, validation, and deployment.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"What is your experience with Microsoft Office, specifically Excel, Word, PowerPoint, and Power BI?\",\n",
            "        \"answer\": \"In my experience, Microsoft Office has been a critical toolset that I have used extensively throughout my academic and professional career. Specifically, I have used Excel, Word, PowerPoint, and Power BI in various capacities.\\n\\nIn Excel, for instance, I have used it for data analysis during my internship at Watad Energy & Communications Co. I used Excel's advanced formulas and functions to manage, sort, and analyze large sets of data, which helped us identify key trends and patterns. This analysis was instrumental in making informed decisions and strategies for the company.\\n\\nIn terms of Word, I have used it extensively for report writing and documentation. During my Bachelor's degree in Computer Science at King Faisal University, I used Word to write and format my project reports, ensuring they were professional and well-structured.\\n\\nPowerPoint has been my go-to tool for presentations. During the Data Science and Artificial Intelligence Bootcamp at T5 SDAIA, I used PowerPoint to present my findings and solutions from various projects. I leveraged its features to create visually appealing and impactful presentations that effectively communicated complex data science concepts to a non-technical audience.\\n\\nFinally, Power BI has been a powerful tool for data visualization. In my bootcamp, I built several ML, DL, and NLP models, and Power BI was instrumental in visualizing the data and results. It enabled me to create interactive dashboards and reports, making the data more understandable and accessible to stakeholders.\\n\\nOverall, my experience with Microsoft Office, particularly Excel, Word, PowerPoint, and Power BI, has been comprehensive and varied, equipping me with the necessary skills to effectively analyze data, document findings, present results, and make data-driven decisions.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"Can you provide an example of a complex SQL query you've written and explain its purpose?\",\n",
            "        \"answer\": \"Sure, I can certainly provide an example. \\n\\nSituation: During my internship at Watad Energy & Communications Co, I was given the task to handle a large dataset that contained information about the company's transactions over the past five years. The dataset was extensive and included various details such as transaction ID, date, customer details, product details, and the amount.\\n\\nTask: My task was to identify the top 10 customers who had the highest transactions in terms of amount, in each year. \\n\\nAction: To achieve this, I wrote a complex SQL query using the RANK() function. Here is a simplified version of the query:\\n\\n```sql\\nSELECT year, customer_id, total_amount,\\nRANK() OVER(PARTITION BY year ORDER BY total_amount DESC) as rank\\nFROM (\\nSELECT YEAR(transaction_date) as year, customer_id, SUM(amount) as total_amount\\nFROM transactions\\nGROUP BY year, customer_id) as subquery\\nWHERE rank <= 10;\\n```\\n\\nThis query first groups the transactions by year and customer_id and calculates the sum of the amounts for each group. It then ranks these groups within each year based on the total amount in descending order. The outer query then filters out the groups that have a rank of 10 or less.\\n\\nResult: This query provided me with a concise list of top 10 customers for each year based on the transaction amounts. The results were valuable for the sales and marketing team as they were able to identify their most valuable customers and strategize their plans accordingly.\\n\\nSo, this is an example of how I used a complex SQL query to extract meaningful information from a large dataset.\"\n",
            "      }\n",
            "    ],\n",
            "    \"behavioral_answers\": [\n",
            "      {\n",
            "        \"question\": \"Can you describe a situation where your adaptability was tested and how you handled it?\",\n",
            "        \"answer\": \"Sure, I'd be happy to share an instance where my adaptability was put to the test. This occurred during my time as an intern at Watad Energy & Communications Co. I was initially brought on to assist with general programming tasks using Python and SQL, but a few weeks into my internship, the company decided to pivot and focus on a new project that required extensive use of HTML and CSS, two languages I was less familiar with.\\n\\nIn this situation, the Task was to quickly get up to speed with these languages to contribute to the project effectively. I took the Action of dedicating my personal time to learn HTML and CSS through online courses and practice exercises. I also sought assistance from my colleagues who were proficient in these languages, asking them for tips and best practices.\\n\\nThe Result was that within a couple of weeks, I was able to contribute to the project effectively, and by the end of my internship, I had significantly improved my skills in HTML and CSS. This experience not only tested my adaptability but also reinforced its importance in a dynamic work environment like tech. It was a challenging but rewarding experience that has prepared me for future situations where I might need to quickly learn and adapt.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"How do you handle disagreements within a team? Can you provide an example?\",\n",
            "        \"answer\": \"Handling disagreements within a team is a crucial aspect of teamwork. I believe the key to resolving such issues is open communication, understanding, and respect for each other's perspectives. \\n\\nTo provide an example, during my internship at Watad Energy & Communications Co, there was a situation where my team was tasked with developing a new feature for our software. There was a disagreement on the approach to take - some members wanted to use Python while others, including me, thought SQL would be more efficient for our needs. \\n\\nInstead of letting the disagreement escalate, I suggested we have a meeting where each side could present their argument for their preferred programming language. We all agreed to this. In the meeting, we each presented our cases, highlighting the benefits and potential drawbacks of each language for our specific project. \\n\\nAfter hearing each other's perspectives, we realized that both languages had their merits. As a result, we decided to use Python for some parts of the project and SQL for others. This not only resolved our disagreement but also resulted in a more efficient and effective solution.\\n\\nThis experience taught me the importance of listening to others' ideas and finding a common ground, even when disagreements arise. I believe that every team member's opinion is valuable and can lead to better results when considered respectfully.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"What strategies do you use to ensure effective communication when working in a team?\",\n",
            "        \"answer\": \"As a computer science professional with a strong focus on data analysis and machine learning, effective communication is key to my work, especially when working in a team. I believe that the best strategies for effective communication involve a combination of clear and concise messaging, active listening, and fostering a culture of openness.\\n\\nFor instance, during my internship at Watad Energy & Communications Co, I was part of a project that involved developing a data analysis model. The team was diverse, with members having different skill sets and backgrounds. To ensure everyone was on the same page, I initiated the use of a shared project management tool where we could all update our progress and share insights. This not only kept everyone informed but also promoted transparency and accountability.\\n\\nMoreover, I've found that regular check-ins and meetings are crucial for effective team communication. During the Data Science and Artificial Intelligence Bootcamp at T5 SDAIA, we had daily stand-up meetings where each team member would share their updates, challenges, and next steps. This encouraged active listening and made sure everyone was aligned with the project goals and timelines.\\n\\nLastly, I strongly believe in the power of asking questions and encouraging others to do the same. This not only helps to clarify any misunderstandings but also promotes a culture of curiosity and learning. For example, in my programming projects, if a team member didn't understand a piece of code I wrote, I would take the time to explain it, and also ask for their input to make it more efficient or effective.\\n\\nOverall, I believe that effective communication is a dynamic process that requires continuous effort, especially in a team setting. It's about creating an environment where everyone feels heard, understood, and valued.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"Can you describe a situation where you had to learn a new skill or technology quickly in order to complete a task or project?\",\n",
            "        \"answer\": \"Sure, I can definitely share an example. During my internship at Watad Energy & Communications Co, I was tasked with creating an application that would streamline the process of data analysis for the team. While I had a solid foundation in Python, SQL, CSS, and HTML, I wasn't familiar with the specific data analysis library that the company used.\\n\\nRecognizing the urgency of the task, I immediately took the initiative to learn the library. I started by dedicating a few hours each day to understand its workings and functionalities. I read through the documentation, followed online tutorials, and even participated in relevant forums. In addition, I also made sure to apply what I was learning by creating small projects using the library.\\n\\nDespite the steep learning curve, my adaptability and determination helped me master the tool within a short span of time. Eventually, I was able to successfully build the application which significantly improved the efficiency of data analysis in the company. This experience truly reinforced my belief in the importance of continuous learning and adaptability in the world of technology.\"\n",
            "      },\n",
            "      {\n",
            "        \"question\": \"Can you provide an example of a challenging situation you faced during your internships and how you overcame it?\",\n",
            "        \"answer\": \"Sure, during my internship at Watad Energy & Communications Co, I was given a project that required me to analyze a large dataset and use my programming skills to identify patterns and trends. The challenge was that the dataset was quite complex and messy, with missing and inconsistent data. \\n\\nTo overcome this challenge, I used the Situation, Task, Action, Result (STAR) method. \\n\\nSituation: The dataset I was given was quite large and had a lot of missing and inconsistent data. This made it difficult to analyze and draw meaningful conclusions from it.\\n\\nTask: My task was to clean the dataset and analyze it to identify patterns and trends that could help the company make informed business decisions.\\n\\nAction: I used my programming skills in Python and SQL to clean the data and fill in the missing values. I also used my data analysis skills to identify inconsistencies in the data and correct them. I then built Machine Learning models to identify patterns and trends in the data.\\n\\nResult: After cleaning the data and analyzing it, I was able to identify several key trends that were previously unnoticed. My findings were well received by the company and they were able to use my analysis to make informed business decisions. This experience taught me the importance of being adaptable and using my technical skills to overcome challenges.\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"filename\": \"RawanAlnajim_cv.pdf\",\n",
            "    \"uploaded_at\": \"2025-05-03 20:02:25.584214\",\n",
            "    \"raw_text\": \"R A -N AWAN L AJIM Saudi Arabia \\u2666 966551999208 \\u2666 alnajimrm@gmail.com \\u2666 www.linkedin.com/in/rawan-alnajim-67ab07271 PROFESSIONAL SUMMARY Fresh computer science graduate, passionate about artificial intelligence. I joined the SDAIA Data Science and Artificial Intelligence Bootcamp to boost my skills. Now, I'm excited to kickstart my career, using what I've learned to tackle real-world problems. I'm eager to work with others, keep learning, and stay up to date with new tech. CERTIFICATIONS \\uf0b7 DGX Virtual Installation Training Curriculum. \\uf0b7 Internship Trainee Watad Energy & Communications Co. \\uf0b7 Certificate in Supervised Machine Learning: Regression and Classification (Apr 24, 2024). \\uf0b7 SDAIA T5 Data Science & AI Bootcamp certificat (3/2024-6/2024). SKILLS \\u2022 Teamwork \\uf0b7 Programming languages python, SQL, CSS and HTML \\u2022 Easily Adaptable \\uf0b7 Data Analysis skills \\u2022 Social \\uf0b7 Microsoft office (Excel, Word, PowerPoint, and power bi) \\uf0b7 Build ML, DL and NLP models. EXPERIENCE Intern at Watad Energy & Communications Co - Alkhobar, JUL 2023 - AUG 2023 \\uf0b7 Contributed to the development of the Arabic ChatGPT project \\\"Mulhem,\\\" emphasizing the critical role of data collection in AI systems. \\uf0b7 Acquired practical experience in data collection methodologies, recognizing its significance in training AI algorithms and models. Data Science and Artificial Intelligence Bootcamp T5 SDAIA - Ryiadh, MAR 2024-JUN 2024 \\uf0b7 I acquired expertise in SQL, data analysis, machine learning, deep learning, computer vision, natural language processing, and generative AI \\uf0b7 Created and implemented a computer vision and deep learning project to assess coral reef health. Utilized YOLOv5, CNN models for coral classification and fish detection. Enhanced coral reef monitoring for global marine conservation efforts. EDUCATION Bachelor: Bachelor of Computer Science , 12/2023 King Faisal University - Saudi Arabia LANGUAGES Arabic English\",\n",
            "    \"parsed_profile\": \"{\\n  \\\"Skills\\\": [\\n    \\\"Teamwork\\\",\\n    \\\"Programming languages python, SQL, CSS and HTML\\\",\\n    \\\"Easily Adaptable\\\",\\n    \\\"Data Analysis skills\\\",\\n    \\\"Social\\\",\\n    \\\"Microsoft office (Excel, Word, PowerPoint, and power bi)\\\",\\n    \\\"Build ML, DL and NLP models\\\"\\n  ],\\n  \\\"Education\\\": [\\n    {\\n      \\\"Degree\\\": \\\"Bachelor of Computer Science\\\",\\n      \\\"Year\\\": \\\"12/2023\\\",\\n      \\\"Institution\\\": \\\"King Faisal University - Saudi Arabia\\\"\\n    }\\n  ],\\n  \\\"Experience\\\": [\\n    {\\n      \\\"Job Title\\\": \\\"Intern\\\",\\n      \\\"Company\\\": \\\"Watad Energy & Communications Co - Alkhobar\\\",\\n      \\\"Years\\\": \\\"JUL 2023 - AUG 2023\\\"\\n    },\\n    {\\n      \\\"Job Title\\\": \\\"Data Science and Artificial Intelligence Bootcamp\\\",\\n      \\\"Company\\\": \\\"T5 SDAIA - Ryiadh\\\",\\n      \\\"Years\\\": \\\"MAR 2024-JUN 2024\\\"\\n    }\\n  ]\\n}\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers, session_data = run_rag_workflow(file_path, vectorstore=vectorstore)\n",
        "\n",
        "questions_json = generate_interview_questions(session_data[\"parsed_profile\"])\n",
        "question_dict = json.loads(questions_json)\n",
        "\n",
        "print(\"🛠️ Technical Questions:\")\n",
        "for q in question_dict[\"technical_questions\"]:\n",
        "    print(\"-\", q)\n",
        "\n",
        "print(\"\\n🧠 Behavioral Questions:\")\n",
        "for q in question_dict[\"behavioral_questions\"]:\n",
        "    print(\"-\", q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN9e-4yTEAcF",
        "outputId": "658c558a-0d4b-4e19-d547-68f186711169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛠️ Technical Questions:\n",
            "- Can you explain a project where you utilized Python, SQL, CSS, and HTML together?\n",
            "- How would you approach a data analysis task? Can you provide an example of a complex data analysis you've performed?\n",
            "- Could you describe your experience with building ML, DL, and NLP models? What challenges did you face and how did you overcome them?\n",
            "- Can you describe your proficiency with Microsoft Office, specifically Excel, Word, PowerPoint, and Power BI? How have you used these tools in your past roles?\n",
            "- During your Data Science and Artificial Intelligence Bootcamp, what was the most challenging project you worked on and why?\n",
            "\n",
            "🧠 Behavioral Questions:\n",
            "- Can you provide an example of a time when you had to adapt quickly to a new situation or environment?\n",
            "- Describe a situation where you had to work as part of a team. What was your role and how did you contribute to the team's success?\n",
            "- How do you handle feedback and criticism? Can you provide an example?\n",
            "- During your internship at Watad Energy & Communications Co, what was a significant challenge you faced and how did you overcome it?\n",
            "- Can you describe a situation where your social skills significantly contributed to your success in a project or task?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2-BQHtMIIzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}